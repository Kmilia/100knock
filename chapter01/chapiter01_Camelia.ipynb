{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 00. Reversed string\n",
    "input_str = \"stressed\"\n",
    "concatenated_str = input_str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 01. \"schooled\"\n",
    "input_str = \"schooled\"\n",
    "concatenated_str = input_str[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 02. \"shoe\" + \"cold\" = \"schooled\"\n",
    "input1_str = \"shoe\"\n",
    "input2_str = \"cold\"\n",
    "\n",
    "min_length = min(len(input1_str), len(input2_str))\n",
    "concatenated_str = \"\".join([input1_str[i]+input2_str[i] for i in range(min_length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 03. Pi\n",
    "SENTENCE = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics\"\n",
    "\n",
    "tokens_list = SENTENCE.replace(\",\", \"\").split(\" \")\n",
    "numbers_list = []\n",
    "\n",
    "for token in tokens_list:\n",
    "    numbers_list.append(len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 04. Atomic symbols\n",
    "ONE_LETTER_EXTRACT = (1, 5, 6, 7, 8, 9, 15, 16, 19)\n",
    "SENTENCE = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can\"\n",
    "\n",
    "tokens_list = SENTENCE.replace(\".\", \"\").split(\" \")\n",
    "extracted_map = {}\n",
    "\n",
    "for token_index in range(len(tokens_list)):\n",
    "    natural_index = token_index + 1 \n",
    "    if natural_index in ONE_LETTER_EXTRACT:\n",
    "        extracted_map[tokens_list[token_index][0]] = natural_index\n",
    "    else:\n",
    "        extracted_map[tokens_list[token_index][0:2]] = natural_index\n",
    "        \n",
    "extracted_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 05. N-gram\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_word_ngrams(sequence: object, N: int) -> list:\n",
    "    if type(sequence) is str:\n",
    "        sequence = sequence.split(\" \")\n",
    "    return list(ngrams(sequence, N))\n",
    "\n",
    "def generate_letter_ngrams(sequence: object, N: int) -> list:\n",
    "    if type(sequence) is str:\n",
    "        sequence = sequence.replace(\" \", \"\")\n",
    "    return list(ngrams(sequence, N))\n",
    "\n",
    "N = int(input(\"Enter how many n-grams you want: \"))\n",
    "sequence_input = \"I am an NLPer\" #[\"He\", \"ll\", \"o\", \"Wo\", \"rd\"]\n",
    "\n",
    "word_ngrams = generate_word_ngrams(sequence_input,N)\n",
    "letter_ngrams = generate_letter_ngrams(sequence_input,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 06. Set\n",
    "WORD1 = \"paraparaparadise\"\n",
    "WORD2 = \"paragraph\"\n",
    "SE_BIGRAM = (\"s\",\"e\")\n",
    "N = 2\n",
    "\n",
    "### N-GRAM GENERATION ###\n",
    "ngrams_set1 = generate_letter_ngrams(WORD1, N)\n",
    "ngrams_set2 = generate_letter_ngrams(WORD2, N)\n",
    "    \n",
    "### SET MANIPULATION ###\n",
    "union = list(set(ngrams_set1).union(ngrams_set2))\n",
    "intersection = list(set(ngrams_set1).intersection(ngrams_set2))\n",
    "set1_diff_set2 = list(set(ngrams_set1).difference(ngrams_set2))\n",
    "set2_diff_set1 = list(set(ngrams_set2).difference(ngrams_set1))\n",
    "se2gram_in_set1_bool = (SE_BIGRAM in ngrams_set1)\n",
    "se2gram_in_set2_bool = (SE_BIGRAM in ngrams_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 07. Template-based senyence generation\n",
    "\n",
    "def make_sentence(x: object, y: object, z: object) -> str:\n",
    "    sentence = str(y) + \" is \" + str(z) + \" at \" + str(x)\n",
    "    return sentence\n",
    "\n",
    "x = 12\n",
    "y = \"temperature\"\n",
    "z = 22.4\n",
    "\n",
    "print(make_sentence(x, y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 08. Cipher Text\n",
    "\n",
    "def crypt(message: str) -> list:\n",
    "    cipher = \"\"\n",
    "    for letter in message:\n",
    "        if letter.islower():\n",
    "            letter = chr(219 - ord(letter))\n",
    "        cipher += letter\n",
    "    return cipher\n",
    "\n",
    "def decrypt(message: str) -> list:\n",
    "    return crypt(message)\n",
    "\n",
    "message = \"H3llo it's Me!\"\n",
    "cipher = crypt(message)\n",
    "decipher = decrypt(cipher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Snewod udner rruio '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 09. Typoglycemia\n",
    "\n",
    "import random\n",
    "\n",
    "def typoglycemia(words_sequence: str) -> str:\n",
    "    words_list = words_sequence.split(\" \")\n",
    "    new_words_sequence = \"\"\n",
    "    LIMIT = 4\n",
    "\n",
    "    for word in words_list:\n",
    "        word_length = len(word)\n",
    "        if word_length > LIMIT:\n",
    "            tail_index = word_length-1\n",
    "            middle_letters = ''.join(random.sample(word[1:tail_index],tail_index-1))\n",
    "            word = word[0] + middle_letters + word[tail_index]\n",
    "        new_words_sequence += word + \" \"\n",
    "\n",
    "    return new_words_sequence\n",
    "\n",
    "words_sequence = \"I couldnâ€™t believe that I could actually understand what I was reading : the phenomenal power of the human mind\"\n",
    "typoglycemia(words_sequence)\n",
    "\n",
    "typoglycemia(\"Snowed under rurio\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
